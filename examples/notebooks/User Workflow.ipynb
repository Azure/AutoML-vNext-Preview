{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121a2d97",
   "metadata": {},
   "source": [
    "First we fetch the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ac153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = shap.datasets.adult()\n",
    "print(\"Data fetched\")\n",
    "target_feature = \"income\"\n",
    "y = [1 if y_i else 0 for y_i in y]\n",
    "\n",
    "full_data = X.copy()\n",
    "full_data[target_feature] = y\n",
    "\n",
    "data_train, data_test = train_test_split(\n",
    "    full_data, test_size=1000, random_state=96132, stratify=full_data[target_feature]\n",
    ")\n",
    "\n",
    "# Don't write out the row indices to the CSV.....\n",
    "print(\"Saving to files\")\n",
    "data_train.to_parquet(\"adult_train.parquet\", index=False)\n",
    "data_test.to_parquet(\"adult_test.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec3192b",
   "metadata": {},
   "source": [
    "Now create an MLClient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d6e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = 'SUB_ID'\n",
    "resource_group = 'RG_NAME'\n",
    "workspace_name = 'WS_NAME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "ml_client = MLClient(credential=DefaultAzureCredential(exclude_shared_token_cache_credential=True),\n",
    "                     subscription_id=subscription_id,\n",
    "                     resource_group_name=resource_group,\n",
    "                     workspace_name=workspace_name,\n",
    "                     logging_enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b06c44",
   "metadata": {},
   "source": [
    "Upload the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a309cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Dataset\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    name=\"Adult_Train_from_Notebook\",\n",
    "    local_path=\"adult_train.parquet\",\n",
    ")\n",
    "ml_client.datasets.create_or_update(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30747ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(\n",
    "    name=\"Adult_Test_from_Notebook\",\n",
    "    local_path=\"adult_test.parquet\",\n",
    ")\n",
    "ml_client.datasets.create_or_update(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f72fb",
   "metadata": {},
   "source": [
    "# Creating the Model\n",
    "\n",
    "To simplify the model creation process, we're going to use a pipeline.\n",
    "\n",
    "Before we do anything else, we need to specify the version of the RAI components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_string = '1639421365'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce55f3e6",
   "metadata": {},
   "source": [
    "Now we can create the training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training_script.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--training_data\", type=str, help=\"Path to training data\")\n",
    "    parser.add_argument(\"--target_column_name\", type=str, help=\"Name of target column\")\n",
    "    parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    current_experiment = Run.get_context().experiment\n",
    "    tracking_uri = current_experiment.workspace.get_mlflow_tracking_uri()\n",
    "    print(\"tracking_uri: {0}\".format(tracking_uri))\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(current_experiment.name)\n",
    "\n",
    "    # Read in data\n",
    "    print(\"Reading data\")\n",
    "    all_data = pd.read_parquet(args.training_data)\n",
    "\n",
    "    print(\"Extracting X_train, y_train\")\n",
    "    print(\"all_data cols: {0}\".format(all_data.columns))\n",
    "    y_train = all_data[args.target_column_name]\n",
    "    X_train = all_data.drop(labels=args.target_column_name, axis=\"columns\")\n",
    "    print(\"X_train cols: {0}\".format(X_train.columns))\n",
    "\n",
    "    print(\"Training model\")\n",
    "    # The estimator can be changed to suit\n",
    "    model = LogisticRegression(solver=\"liblinear\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Saving model with mlflow - leave this section unchanged\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        print(\"Saving model with MLFlow to temporary directory\")\n",
    "        tmp_output_dir = os.path.join(td, \"my_model_dir\")\n",
    "        mlflow.sklearn.save_model(sk_model=model, path=tmp_output_dir)\n",
    "\n",
    "        print(\"Copying MLFlow model to output path\")\n",
    "        for file_name in os.listdir(tmp_output_dir):\n",
    "            print(\"  Copying: \", file_name)\n",
    "            # As of Python 3.8, copytree will acquire dirs_exist_ok as\n",
    "            # an option, removing the need for listdir\n",
    "            shutil.copy2(src=os.path.join(tmp_output_dir, file_name), dst=os.path.join(args.model_output, file_name))\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704878b",
   "metadata": {},
   "source": [
    "Now, we want to place this into a component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Code, CommandComponent\n",
    "\n",
    "training_code = Code(\n",
    "    local_path='training_script.py'\n",
    ")\n",
    "\n",
    "training_inputs = {\n",
    "    'training_data': { 'type': 'path'},\n",
    "    'target_column_name': { 'type': 'string'}\n",
    "}\n",
    "\n",
    "training_outputs = {\n",
    "    'model_output': { 'type': 'path'}\n",
    "}\n",
    "\n",
    "training_component = CommandComponent(\n",
    "    name=\"MyTrainingComponent\",\n",
    "    version=\"2\",\n",
    "    display_name=\"Simple training component\",\n",
    "    code=training_code,\n",
    "    environment=f\"AML-RAI-Environment:{version_string}\",\n",
    "    inputs=training_inputs,\n",
    "    outputs=training_outputs,\n",
    "    command=\"python training_script.py \" \\\n",
    "            \"--training_data ${{inputs.training_data}} \" \\\n",
    "            \"--target_column_name ${{inputs.target_column_name}} \" \\\n",
    "            \"--model_output ${{outputs.model_output}}\"\n",
    ")\n",
    "\n",
    "ml_client.components.create_or_update(training_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83cf52f",
   "metadata": {},
   "source": [
    "# Running a training pipeline\n",
    "Now we have a script which can train a model, we need to run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from azure.ml.entities import JobInput, ComponentJob, PipelineJob\n",
    "\n",
    "model_name_suffix = int(time.time())\n",
    "model_name = 'my_trained_nb_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e6ff94",
   "metadata": {},
   "source": [
    "This is going to be a two component pipeline. The first will be the one we created above, which will train our model. The second will register it in AzureML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f93027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The overall inputs for the pipeline\n",
    "\n",
    "pipeline_inputs = {\n",
    "    'target_column_name': 'income',\n",
    "    'my_training_data': JobInput(dataset=f\"Adult_Train_from_Notebook:1\"),\n",
    "    'my_test_data': JobInput(dataset=f\"Adult_Test_from_Notebook:1\")\n",
    "}\n",
    "\n",
    "# Specify the training job\n",
    "train_job_inputs = {\n",
    "    'target_column_name': '${{inputs.target_column_name}}',\n",
    "    'training_data': '${{inputs.my_training_data}}',\n",
    "}\n",
    "train_job_outputs = {\n",
    "    'model_output': None\n",
    "}\n",
    "train_job = ComponentJob(\n",
    "    component=f\"MyTrainingComponent:2\",\n",
    "    inputs=train_job_inputs,\n",
    "    outputs=train_job_outputs\n",
    ")\n",
    "\n",
    "# The model registration job\n",
    "register_job_inputs = {\n",
    "    'model_input_path': '${{jobs.train-model-job.outputs.model_output}}',\n",
    "    'model_base_name': model_name,\n",
    "    'model_name_suffix': model_name_suffix\n",
    "}\n",
    "register_job_outputs = {\n",
    "    'model_info_output_path': None\n",
    "}\n",
    "register_job = ComponentJob(\n",
    "    component=f\"RegisterModel:{version_string}\",\n",
    "    inputs=register_job_inputs,\n",
    "    outputs=register_job_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc9970f",
   "metadata": {},
   "source": [
    "With our jobs specified, assemble them into a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2410db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registration_pipeline_job = PipelineJob(\n",
    "    experiment_name=f\"Register_Model_From_Notebook_01\",\n",
    "    description=\"Create and register a model from a notebook\",\n",
    "    jobs={\n",
    "        'train-model-job': train_job,\n",
    "        'register-model-job': register_job,\n",
    "    },\n",
    "    inputs=pipeline_inputs,\n",
    "    outputs=register_job_outputs,\n",
    "    compute=\"cpucluster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a6422",
   "metadata": {},
   "source": [
    "And submit it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d450922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import PipelineJob\n",
    "\n",
    "def submit_and_wait(ml_client, pipeline_job) -> PipelineJob:\n",
    "    created_job = ml_client.jobs.create_or_update(pipeline_job)\n",
    "    assert created_job is not None\n",
    "\n",
    "    while created_job.status not in ['Completed', 'Failed', 'Canceled', 'NotResponding']:\n",
    "        time.sleep(30)\n",
    "        created_job = ml_client.jobs.get(created_job.name)\n",
    "        print(\"Latest status : {0}\".format(created_job.status))\n",
    "    assert created_job.status == 'Completed'\n",
    "    return created_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42543ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the actual submission\n",
    "\n",
    "training_job = submit_and_wait(ml_client, model_registration_pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3779f9e4",
   "metadata": {},
   "source": [
    "# Creating the RAI Insights\n",
    "We have a registered model, and can now run a pipeline to create the RAI insights. First off, compute the name of the model we registered (this is not straightforward since the Register Model component is used in testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b893bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_model_id = f'{model_name}_{model_name_suffix}:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8c009",
   "metadata": {},
   "source": [
    "Now, we create the RAI pipeline itself. There are three 'component stages' in this pipeline:\n",
    "\n",
    "1. Fetch the model\n",
    "1. Construct an empty RAI dashboard\n",
    "1. Run the RAI tool components\n",
    "\n",
    "The job to fetch the registered model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This won't be necessary once models are types within the pipeline graph\n",
    "\n",
    "fetch_job_inputs = {\n",
    "    'model_id': expected_model_id\n",
    "}\n",
    "fetch_job_outputs = {\n",
    "    'model_info_output_path': None\n",
    "}\n",
    "fetch_job = ComponentJob(\n",
    "    component=f\"FetchRegisteredModel:{version_string}\",\n",
    "    inputs=fetch_job_inputs,\n",
    "    outputs=fetch_job_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b192f",
   "metadata": {},
   "source": [
    "With this registered model (and our datasets), we can create an empty RAI dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level RAI Insights component\n",
    "\n",
    "# We will reuse the same pipeline_inputs object in the end\n",
    "create_rai_inputs = {\n",
    "    'title': 'Run built from a Notebook',\n",
    "    'task_type': 'classification',\n",
    "    'model_info_path': '${{jobs.fetch-model-job.outputs.model_info_output_path}}',\n",
    "    'train_dataset': '${{inputs.my_training_data}}',\n",
    "    'test_dataset': '${{inputs.my_test_data}}',\n",
    "    'target_column_name': '${{inputs.target_column_name}}',\n",
    "    'categorical_column_names': '[\"Race\", \"Sex\", \"Workclass\", \"Marital Status\", \"Country\", \"Occupation\"]',\n",
    "}\n",
    "create_rai_outputs = {\n",
    "    'rai_insights_dashboard': None # Could theoretically redirect the datastore here\n",
    "}\n",
    "create_rai_job = ComponentJob(\n",
    "    component=f\"RAIInsightsConstructor:{version_string}\",\n",
    "    inputs=create_rai_inputs,\n",
    "    outputs=create_rai_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff017fa",
   "metadata": {},
   "source": [
    "Now, create an instance of each of our RAI tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the explanation\n",
    "\n",
    "# Not explanations, pending bugfix for save/load/save scenario\n",
    "# explain_inputs = {\n",
    "#    'comment': 'Insert text here',\n",
    "#    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}'\n",
    "#}\n",
    "#explain_outputs = {\n",
    "#    'explanation': None\n",
    "#}\n",
    "#explain_job = ComponentJob(\n",
    "#    component=f\"RAIInsightsExplanation:{version_string}\",\n",
    "#    inputs=explain_inputs,\n",
    "#    outputs=explain_outputs\n",
    "#)\n",
    "\n",
    "# Setup causal\n",
    "causal_inputs = {\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    'treatment_features': '[\"Age\", \"Sex\"]',\n",
    "    'heterogeneity_features': '[\"Marital Status\"]'\n",
    "}\n",
    "causal_outputs = {\n",
    "    'causal': None\n",
    "}\n",
    "causal_job = ComponentJob(\n",
    "    component=f\"RAIInsightsCausal:{version_string}\",\n",
    "    inputs=causal_inputs,\n",
    "    outputs=causal_outputs\n",
    ")\n",
    "\n",
    "# Setup counterfactual\n",
    "counterfactual_inputs = {\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    'total_CFs': '10',\n",
    "    'desired_class': 'opposite'\n",
    "}\n",
    "counterfactual_outputs = {\n",
    "    'counterfactual': None\n",
    "}\n",
    "counterfactual_job = ComponentJob(\n",
    "    component=f\"RAIInsightsCounterfactual:{version_string}\",\n",
    "    inputs=counterfactual_inputs,\n",
    "    outputs=counterfactual_outputs\n",
    ")\n",
    "\n",
    "# Setup error analysis\n",
    "error_analysis_inputs = {\n",
    "    'rai_insights_dashboard': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    'filter_features': '[\"Race\", \"Sex\", \"Workclass\", \"Marital Status\", \"Country\", \"Occupation\"]'\n",
    "}\n",
    "error_analysis_outputs = {\n",
    "    'error_analysis': None\n",
    "}\n",
    "error_analysis_job = ComponentJob(\n",
    "    component=f\"RAIInsightsErrorAnalysis:{version_string}\",\n",
    "    inputs=error_analysis_inputs,\n",
    "    outputs=error_analysis_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d98393",
   "metadata": {},
   "source": [
    "Now the 'gather' component which assembles everything into an `RAIInsights` object, and computes the JSON for the UX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c71bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the gather component\n",
    "gather_inputs = {\n",
    "    'constructor': '${{jobs.create-rai-job.outputs.rai_insights_dashboard}}',\n",
    "    # 'insight_1': '${{jobs.explain-rai-job.outputs.explanation}}',\n",
    "    'insight_2': '${{jobs.causal-job.outputs.causal}}',\n",
    "    'insight_3': '${{jobs.counterfactual-job.outputs.counterfactual}}',\n",
    "    'insight_4': '${{jobs.error-analysis-job.outputs.error_analysis}}'\n",
    "}\n",
    "gather_outputs = {\n",
    "    'dashboard': None,\n",
    "    'ux_json': None\n",
    "}\n",
    "gather_job = ComponentJob(\n",
    "    component=f\"RAIInsightsGather:{version_string}\",\n",
    "    inputs=gather_inputs,\n",
    "    outputs=gather_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db19bf2",
   "metadata": {},
   "source": [
    "Finally, the pipeline itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4639a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to construct the RAI Insights\n",
    "insights_pipeline_job = PipelineJob(\n",
    "    experiment_name=f\"Compute_Insights_from_Notebook_{version_string}\",\n",
    "    description=\"Python submitted Adult insights using fetched model\",\n",
    "    jobs={\n",
    "        'fetch-model-job': fetch_job,\n",
    "        'create-rai-job': create_rai_job,\n",
    "        'causal-job': causal_job,\n",
    "        'counterfactual-job': counterfactual_job,\n",
    "        'error-analysis-job': error_analysis_job,\n",
    "        # 'explain-job': explain_job,\n",
    "        'gather-job': gather_job\n",
    "    },\n",
    "    inputs=pipeline_inputs,\n",
    "    outputs=None,\n",
    "    compute=\"cpucluster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fff73b",
   "metadata": {},
   "source": [
    "And submit it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a4915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "insights_job = submit_and_wait(ml_client, insights_pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b63f92",
   "metadata": {},
   "source": [
    "# Download and display the insights\n",
    "Now we can download the insights we have computed. To start, we need to obtain the Run id of the 'gather-job' which ran as part of the previous pipeline. We have a helper for this, but the name of the experiment is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b36de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_ml_rai._list_rai_runs import list_rai_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27324a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_list = list_rai_insights(ml_client, insights_pipeline_job.experiment_name)\n",
    "\n",
    "print(insights_pipeline_job.experiment_name)\n",
    "display(run_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add2092",
   "metadata": {},
   "source": [
    "Before we can download the generated insights, we need to go to the Azure portal, and locate the blob storage account associated with the workspace. Within the 'Access Control' pane, we then add ourselves to the role 'Storage Blob Data Contributor.'\n",
    "\n",
    "With this done (and a small pause to ensure permissions have propagated), we can use the mini SDK to download to a local directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_ml_rai._download_rai_insights import download_rai_insights\n",
    "\n",
    "download_dir = 'my_downloaded_insight'\n",
    "\n",
    "download_rai_insights(\n",
    "    ml_client,\n",
    "    rai_insight_id=run_list[0],\n",
    "    path=download_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4abe2d0",
   "metadata": {},
   "source": [
    "And with everything downloaded, we can load the RAIInsights object and instantiate the dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0459d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibleai import RAIInsights\n",
    "from raiwidgets import ResponsibleAIDashboard\n",
    "\n",
    "rai_i = RAIInsights.load(download_dir)\n",
    "\n",
    "ResponsibleAIDashboard(rai_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd0e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
